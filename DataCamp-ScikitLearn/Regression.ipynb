{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Regression.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"b-zwuMnzRsu3","colab_type":"text"},"source":["# Regression Section"]},{"cell_type":"markdown","metadata":{"id":"ZT3RbvQ093Uj","colab_type":"text"},"source":["Notice the differences in shape before and after applying the .reshape() method. Getting the feature and target variable arrays into the right format for scikit-learn is an important precursor to model building."]},{"cell_type":"code","metadata":{"id":"ASyH1L_c9t70","colab_type":"code","colab":{}},"source":["# Import numpy and pandas\n","import numpy as np\n","import pandas as pd\n","\n","# Read the CSV file into a DataFrame: df\n","df = pd.read_csv('gapminder.csv')\n","\n","# Create arrays for features and target variable\n","y = df['life'].values\n","X = df['fertility'].values\n","\n","# Print the dimensions of X and y before reshaping\n","print(\"Dimensions of y before reshaping: {}\".format(y.shape))\n","print(\"Dimensions of X before reshaping: {}\".format(X.shape))\n","\n","# Reshape X and y\n","y = y.reshape(-1,1)\n","X = X.reshape(-1,1)\n","\n","# Print the dimensions of X and y after reshaping\n","print(\"Dimensions of y after reshaping: {}\".format(y.shape))\n","print(\"Dimensions of X after reshaping: {}\".format(X.shape))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w2rI1i5sRUJE","colab_type":"text"},"source":["The loss fuinction. Minimize the vertical distance between the line and the points. this distance is called the residual. \n","\n","Minimize the sum of the square of the residuals. Ordinary least squares (OLS): Minimize sum of squares of residuals. "]},{"cell_type":"markdown","metadata":{"id":"m_9HyMODT5xP","colab_type":"text"},"source":["**Ejemplo de Regresion Importante**"]},{"cell_type":"code","metadata":{"id":"AAgtwO2DR0Cu","colab_type":"code","colab":{}},"source":["# Import LinearRegression\n","from sklearn.linear_model import LinearRegression\n","\n","# Create the regressor: reg\n","reg = LinearRegression()\n","\n","# Create the prediction space\n","prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\n","\n","# Fit the model to the data\n","reg.fit(X_fertility, y)\n","\n","# Compute predictions over the prediction space: y_pred\n","y_pred = reg.predict(prediction_space)\n","\n","# Print R^2 \n","print(reg.score(X_fertility, y))\n","\n","# Plot regression line\n","plt.plot(prediction_space, y_pred, color='black', linewidth=3)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PF8T_bFNXSnC","colab_type":"text"},"source":["**Otro Ejemplo de Regression Importante- Esta vez sacando el MSE**"]},{"cell_type":"code","metadata":{"id":"TPkIK_krXYSg","colab_type":"code","colab":{}},"source":["# Import necessary modules\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","\n","# Create training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, \n","                                                    y,\n","                                                    test_size = 0.3,\n","                                                    random_state=42)\n","\n","# Create the regressor: reg_all\n","reg_all = LinearRegression()\n","# Fit the regressor to the training data\n","reg_all.fit(X_train, y_train)\n","# Predict on the test data: y_pred\n","y_pred = reg_all.predict(X_test)\n","\n","# Compute and print R^2 and RMSE\n","print(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","print(\"Root Mean Squared Error: {}\".format(rmse))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Nn6412MXkiI","colab_type":"text"},"source":["Cross Validation- \n","- Model performance is dependent on the way data is split\n","- values of R-squares (five-fold cross-validation)\n","- more fold = More computationally expensive \n","\n","Performing cross validation in scikit learn avoids the dependency of your model on  how the  data is splited\n","\n","The score reported on cross-validation is R squared,  as this is the default score for linear regression. "]},{"cell_type":"code","metadata":{"id":"YUxce_7gYG6D","colab_type":"code","colab":{}},"source":["# Import the necessary modules\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import cross_val_score\n","# Create a linear regression object: reg\n","reg = LinearRegression()\n","# Compute 5-fold cross-validation scores: cv_scores\n","cv_scores = cross_val_score(reg, X,y, cv=5)\n","# Print the 5-fold cross-validation scores\n","print(cv_scores)\n","print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))"],"execution_count":0,"outputs":[]}]}