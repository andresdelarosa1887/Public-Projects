{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_Evaluation.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RJ_5QcAcwiyT","colab_type":"text"},"source":["# Final Evaluation\n","\n","\n","*   How well can the model perform on never before seen data\n","*   Using ALL data for cross-validation is no ideal\n","* split data into training and hold-out set at the beginning \n","* Perform grid searc for cross-validation on training set \n","*  Choose best hyperparameters and evaluate on hold-out set \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"lAci0MIByCOz","colab_type":"code","colab":{}},"source":["# Import necessary modules\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","\n","# Create the hyperparameter grid\n","c_space = np.logspace(-5, 8, 15)\n","param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n","\n","# Instantiate the logistic regression classifier: logreg\n","logreg = LogisticRegression()\n","\n","# Create train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=42)\n","\n","# Instantiate the GridSearchCV object: logreg_cv\n","logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n","\n","# Fit it to the training data\n","logreg_cv.fit(X_train,y_train)\n","\n","# Print the optimal parameters and best score\n","print(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\n","print(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fmzAYp3x1ghk","colab_type":"text"},"source":["# Regression - ElasticNet (other type of Regularization)\n","Remember lasso and ridge regression from the previous chapter? Lasso used the L1 penalty to regularize, while ridge used the L2 penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties:\n","\n","a∗L1+b∗L2\n","In scikit-learn, this term is represented by the 'l1_ratio' parameter: An 'l1_ratio' of 1 corresponds to an L1 penalty, and anything lower is a combination of L1 and L2.\n","\n","In this exercise, you will GridSearchCV to tune the 'l1_ratio' of an elastic net model trained on the Gapminder data. As in the previous exercise, use a hold-out set to evaluate your model's performance."]},{"cell_type":"code","metadata":{"id":"WH1u0qMA1osI","colab_type":"code","colab":{}},"source":["# Import necessary modules\n","from sklearn.linear_model import ElasticNet\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import train_test_split\n","\n","# Create train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X,\n","                y,\n","                test_size=0.4,\n","                random_state=42)\n","\n","# Create the hyperparameter grid\n","l1_space = np.linspace(0, 1, 30)\n","param_grid = {'l1_ratio': l1_space}\n","\n","# Instantiate the ElasticNet regressor: elastic_net\n","elastic_net = ElasticNet()\n","\n","# Setup the GridSearchCV object: gm_cv\n","gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n","\n","# Fit it to the training data\n","gm_cv.fit(X_train, y_train)\n","\n","# Predict on the test set and compute metrics\n","y_pred = gm_cv.predict(X_test)\n","r2 = gm_cv.score(X_test, y_test)\n","mse = mean_squared_error(y_test, y_pred)\n","print(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\n","print(\"Tuned ElasticNet R squared: {}\".format(r2))\n","print(\"Tuned ElasticNet MSE: {}\".format(mse))\n"],"execution_count":0,"outputs":[]}]}