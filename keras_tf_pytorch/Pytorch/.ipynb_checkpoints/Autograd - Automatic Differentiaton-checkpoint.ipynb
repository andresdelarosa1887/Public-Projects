{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "El paquete de autograd es central a todo el backend donde corre PyTorch. Nos permite tener automatic differentiation (que son las técnicas que permiten realizar cálculo numérico aproximado con el ordenador - derivadas, integrales, etc.) y funciona de manera \"define-by-run framework\". Es decir que cada ejecución puede ser diferente.\n",
    "\n",
    "Basado en la documentación:\n",
    "\n",
    "**torch.Tensor** is the central class of the package. If you set its attribute **.requires_grad** as True, it starts to track all operations on it. When you finish your computation you can call **.backward()** and have all the gradients computed automatically. The gradient for this tensor will be accumulated into **.grad** attribute.\n",
    "\n",
    "To stop a tensor from tracking history, you can call **.detach()** to detach it from the computation history, and to prevent future computation from being tracked.\n",
    "\n",
    "To prevent tracking history (and using memory), you can also wrap the code block in **with torch.no_grad():**. This can be particularly helpful when evaluating a model because the model may have trainable parameters with **requires_grad=True**, but for which we don’t need the gradients.\n",
    "\n",
    "There’s one more class which is very important for autograd implementation - a **Function**.\n",
    "\n",
    "**Tensor** and **Function** are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a **.grad_fn** attribute that references a **Function** that has created the **Tensor** (except for Tensors created by the user - their grad_fn is None).\n",
    "\n",
    "If you want to compute the derivatives, you can call .backward() on a Tensor. If Tensor is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to backward(), however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape.\n",
    "\n",
    "Este tutorial está basado en: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\n",
    "\n",
    "Más información sobre autograd en https://pytorch.org/docs/autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos unos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que y ha sido creado por como resultado de una operación, ahora va a tener un **grad_fn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x000001F1C29EA4E0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función .requires_grad_ cambia el atributo .requires_grad_ del tensor. El valor por defecto es falso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000001F1C29EA588>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente\n",
    "Vamos ahora a realizar la operación de backprop (back propagation). Si lo hacemos en un escalar, como la variable \"out\" la operación out.backward() es igual a out.backward(torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27., grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matemáticamente, dado el vector x, su gradiente es la matriz Jacobiana\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "El paquete torch.autograd permite realizar ese cálculo de manera vectorizada y eficaz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -558.7944, -1185.5886,  -899.3242], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"y\" no es ya un escalar, por lo que torch.autograd no puede calcular la matriz Jacobiana completa pero si podemos calcular el producto vector jacobiano. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
