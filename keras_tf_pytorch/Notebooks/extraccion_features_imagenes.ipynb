{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"extraccion_features_imagenes.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kernelspec":{"display_name":"Python [conda env:root] *","language":"python","name":"conda-root-py"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pdn6IUBg9L8c","colab_type":"text"},"source":["# Extracción de features en imagenes usando CNN\n","\n","En este ejemplo se carga un modelo de *convolutional neural network* ya preentrenado para la extracción de *features* de imágenes. En este caso el modelo que se usará es [VGG16](https://arxiv.org/abs/1409.1556) con los pesos entrenados con el dataset [ImageNet](http://www.image-net.org). \n","\n","Los pasos necesarios para la extracción de *features* son:\n","1. Carga del modelo\n","2. Carga de la imagen y ajustes\n","3. Procesamiento de la imagen en la red"]},{"cell_type":"markdown","metadata":{"id":"wb-d1bdj9L8c","colab_type":"text"},"source":["## PASO 1: Carga del modelo\n","En este ejemplo cargamos en la variable model el modelo VGG16. En esta carga se usán los siguientes parámetros:\n","1. `weights='imagenet'`, para indicarle que se usen los pesos obtenidos del entremantiendo mediante ImageNet\n","2. `include_top=False`, mediante este parámetro indicamos que no se quiere añadir las capas completamentes conectadas para la realización de clasificación, que es lo que deseamos para la extracción de *features*, obtener la salida de la última capa antes de la etapa de clasificación, que será la *feature* de alto nivel. En el caso de querer realizar clasificación se debe usar `include_top=True`"]},{"cell_type":"code","metadata":{"id":"ud-Bhhb29L8d","colab_type":"code","colab":{}},"source":["from keras.applications.vgg16 import VGG16\n","from keras.preprocessing import image\n","from keras.applications.vgg16 import preprocess_input\n","import numpy as np\n","\n","model = VGG16(weights='imagenet', include_top=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4JrlXGLf9L8h","colab_type":"text"},"source":["## PASO 2: Carga de la imagen y ajustes\n","Se carga la imagen que se encuentra en la ruta '../datasets/10368.jpg' con el tamaño de imagen con el que es capaz de trabajar la red (número de neuronas de la capa de entrada), el cual es fijo de la arquitectura a usar, en este caso 224x224 px.\n","![calzado](../datasets/10368.jpg)\n","Una vez cargada la imagen, esta se convierte en un vector x, el cual preprocesamos a continuación según las necesidades de la red."]},{"cell_type":"code","metadata":{"id":"1sE_4-2O9L8h","colab_type":"code","colab":{}},"source":["#Dirección en la que se encuentra la imagen\n","img_path = 'datasets/10368.jpg'\n","#Carga de la imagen\n","img = image.load_img(img_path, target_size=(224, 224))\n","#Se convierte la imagen en un vector\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","#Se realiza el preprocesamiento según las caracteristicas de la red\n","x = preprocess_input(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"32dOlpbk9L8k","colab_type":"text"},"source":["## PASO 3: Procesamiento de la imagen en la red\n","Una vez se tiene la imagen cargada de la manera conveniente para su paso a la red, solo es necesario llamar al método `predict` para obtener la salida de esta, siendo en este caso el vector de *features* de alto nivel.\n","Este vector es proporcionado como una array n dimensional, por lo que es necesario convertirlo en un array de dimension 1xN."]},{"cell_type":"code","metadata":{"id":"k2qd5AaI9L8l","colab_type":"code","colab":{}},"source":["#Obtención de las features del modelo\n","featuresND = model.predict(x)\n","features = np.array(featuresND).flatten()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxI-DGluCGgW","colab_type":"code","outputId":"becbcf41-23cb-465a-8ac4-f94e6766d86f","executionInfo":{"status":"ok","timestamp":1559761548322,"user_tz":240,"elapsed":541,"user":{"displayName":"andres de la rosa","photoUrl":"","userId":"10005060268978627133"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["features"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"-Go_3keb9L8n","colab_type":"text"},"source":["En este punto ya tendriamos en la variable `features` el vector con las *features* de alto nivel extraidas de la imágen, usando el modelo de *convolutional neural network* VGG16."]}]}