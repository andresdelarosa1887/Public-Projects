{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de features en imagenes usando CNN\n",
    "\n",
    "En este ejemplo se carga un modelo de *convolutional neural network* ya preentrenado para la extracción de *features* de imágenes. En este caso el modelo que se usará es [VGG16](https://arxiv.org/abs/1409.1556) con los pesos entrenados con el dataset [ImageNet](http://www.image-net.org). \n",
    "\n",
    "Los pasos necesarios para la extracción de *features* son:\n",
    "1. Carga del modelo\n",
    "2. Carga de la imagen y ajustes\n",
    "3. Procesamiento de la imagen en la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 1: Carga del modelo\n",
    "En este ejemplo cargamos en la variable model el modelo VGG16. En esta carga se usán los siguientes parámetros:\n",
    "1. `weights='imagenet'`, para indicarle que se usen los pesos obtenidos del entremantiendo mediante ImageNet\n",
    "2. `include_top=False`, mediante este parámetro indicamos que no se quiere añadir las capas completamentes conectadas para la realización de clasificación, que es lo que deseamos para la extracción de *features*, obtener la salida de la última capa antes de la etapa de clasificación, que será la *feature* de alto nivel. En el caso de querer realizar clasificación se debe usar `include_top=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Mario/anaconda/envs/python3/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 2: Carga de la imagen y ajustes\n",
    "Se carga la imagen que se encuentra en la ruta '../datasets/10368.jpg' con el tamaño de imagen con el que es capaz de trabajar la red (número de neuronas de la capa de entrada), el cual es fijo de la arquitectura a usar, en este caso 224x224 px.\n",
    "![calzado](../datasets/10368.jpg)\n",
    "Una vez cargada la imagen, esta se convierte en un vector x, el cual preprocesamos a continuación según las necesidades de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dirección en la que se encuentra la imagen\n",
    "img_path = '../datasets/10368.jpg'\n",
    "#Carga de la imagen\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "#Se convierte la imagen en un vector\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "#Se realiza el preprocesamiento según las caracteristicas de la red\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 3: Procesamiento de la imagen en la red\n",
    "Una vez se tiene la imagen cargada de la manera conveniente para su paso a la red, solo es necesario llamar al método `predict` para obtener la salida de esta, siendo en este caso el vector de *features* de alto nivel.\n",
    "Este vector es proporcionado como una array n dimensional, por lo que es necesario convertirlo en un array de dimension 1xN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtención de las features del modelo\n",
    "featuresND = model.predict(x)\n",
    "features = np.array(featuresND).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto ya tendriamos en la variable `features` el vector con las *features* de alto nivel extraidas de la imágen, usando el modelo de *convolutional neural network* VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0. -0. -0. ... -0. -0. -0.]\n"
     ]
    }
   ],
   "source": [
    "print features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
