{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio del set de imágenes CIFAR10- Clasificando imágenes diversas\n",
    "- PYTORCH\n",
    "- Andrés de la Rosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importacion de todos los modulos\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "from keras.datasets import cifar10\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta vez cargo el CIFAR10 directamente de los datasets de Keras y no de la pagina de donde estaban originalmente como hice en el ejercicio de TensorFlow. Esto facilitó mucho el trabajo \n",
    "debido a que no tuve que definir las funciones que hacian el preprocesamiento de las imágenes. teniendo en cuenta que al momento de utilizar mis propias imágenes tendré que hacer dicho pre procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 imagenes de entrenamiento\n",
      "10000 imagenes de test\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data() \n",
    "# x_train - training data(images), y_train - labels(digits)\n",
    "print(x_train.shape[0], 'imagenes de entrenamiento')\n",
    "print(x_test.shape[0], 'imagenes de test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convirtiendo a one hot encoding\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#Normalizando la entrada\n",
    "x_train  /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiendo el modelo de 3 capas con relu y max pooling, haciendo el same padding para no perder pixeles\n",
    "model = Sequential()\n",
    "\n",
    "#Primera capa con relu y maxpooling\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#Segunda capa con relu y maxpooling\n",
    "model.add(Conv2D(64, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#Tercera capa con relu y maxpooling\n",
    "model.add(Conv2D(128, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#Poniendo los datos en formato flat\n",
    "model.add(Flatten())\n",
    "\n",
    "#Finalizando con los Fully connected layers\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilamos el modelo para calcular su acierto\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "opt = SGD(lr=0.001, momentum=0.9, decay=1e-6, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Anaconda3\\envs\\practicas\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "40000/40000 [==============================] - 90s 2ms/step - loss: 2.7842 - acc: 0.0940 - val_loss: 2.2797 - val_acc: 0.0535\n",
      "Epoch 2/5\n",
      "40000/40000 [==============================] - 87s 2ms/step - loss: 2.4305 - acc: 0.0831 - val_loss: 2.2902 - val_acc: 0.1257\n",
      "Epoch 3/5\n",
      "40000/40000 [==============================] - 85s 2ms/step - loss: 2.3079 - acc: 0.0809 - val_loss: 2.3157 - val_acc: 0.0378\n",
      "Epoch 4/5\n",
      "40000/40000 [==============================] - 86s 2ms/step - loss: 2.2870 - acc: 0.0596 - val_loss: 2.2332 - val_acc: 0.0420\n",
      "Epoch 5/5\n",
      "40000/40000 [==============================] - 87s 2ms/step - loss: 2.2247 - acc: 0.0481 - val_loss: 3.8114 - val_acc: 0.0318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ae432f86d8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:practicas]",
   "language": "python",
   "name": "conda-env-practicas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
